# AIMS LLM API 设置使用指南

## 🎯 功能概述

AIMS现在支持多个主流AI服务提供商，让您可以根据需求选择最适合的AI模型：

- **OpenAI** 🤖 - GPT-4o, GPT-4o Mini, GPT-3.5 Turbo
- **Google Gemini** 💎 - Gemini 1.5 Pro, Gemini 1.5 Flash  
- **DeepSeek** 🔍 - DeepSeek Chat, DeepSeek Coder
- **Moonshot AI** 🌙 - Moonshot v1 (8K/32K/128K)
- **OpenRouter** 🔀 - Claude 3.5, Llama 3.1, 以及更多模型

## 🚀 快速开始

### 1. 访问LLM设置页面

在AIMS主界面点击右上角的 **"LLM设置"** 按钮，或访问 `http://localhost:5174/` 后点击导航栏中的 🧠 **LLM设置**。

### 2. 添加第一个配置

点击 **"添加配置"** 按钮，填写以下信息：

#### 基础配置
- **服务提供商**：选择您要使用的AI服务
- **自定义名称**：为配置设置易识别的名称（可选）
- **模型ID**：选择预设模型或输入自定义模型ID
- **API Key**：输入您的API密钥
- **API URL**：API端点地址，支持自定义代理或私有部署

#### 高级参数
- **温度** (0-2)：控制输出的随机性，0.7为推荐值
- **最大Token**：单次生成的最大长度，4000为推荐值
- **设为默认**：是否作为系统默认模型
- **立即启用**：是否立即可用

### 3. 测试连接

配置完成后，点击 **"测试连接"** 按钮验证API Key是否有效。

### 4. 编辑配置

对于已有的配置，您可以：

- **查看详情**：点击眼睛图标显示/隐藏API Key
- **编辑配置**：点击编辑图标进入编辑模式
- **保存修改**：在编辑模式下点击保存图标
- **取消编辑**：点击取消图标放弃修改

#### 编辑模式功能
- 修改自定义名称
- 更改模型ID
- 更新API Key
- 自定义API URL
- 调整温度和Token参数

## 📋 各服务商配置指南

### OpenAI 配置

**获取API Key**：
1. 访问 [OpenAI Platform](https://platform.openai.com/)
2. 登录并进入 API Keys 页面
3. 创建新的API Key

**推荐配置**：
```
模型: gpt-4o-mini (性价比高)
温度: 0.7
最大Token: 4000
```

**成本参考**：
- GPT-4o Mini: $0.15/1M输入 + $0.6/1M输出
- GPT-4o: $5/1M输入 + $15/1M输出

### Google Gemini 配置

**获取API Key**：
1. 访问 [Google AI Studio](https://aistudio.google.com/)
2. 创建项目并获取API Key

**推荐配置**：
```
模型: gemini-1.5-flash (快速响应)
温度: 0.7
最大Token: 4000
```

**特点**：
- 超长上下文支持（最高200万tokens）
- 多模态能力
- 成本相对较低

### DeepSeek 配置

**获取API Key**：
1. 访问 [DeepSeek Platform](https://platform.deepseek.com/)
2. 注册并获取API Key

**推荐配置**：
```
模型: deepseek-chat (中文优化)
温度: 0.7
最大Token: 4000
```

**优势**：
- 中文理解能力强
- 成本极低
- 代码生成能力优秀

### Moonshot AI 配置

**获取API Key**：
1. 访问 [Moonshot AI](https://platform.moonshot.cn/)
2. 注册并获取API Key

**推荐配置**：
```
模型: moonshot-v1-32k (平衡性能)
温度: 0.7
最大Token: 4000
```

**特点**：
- Kimi模型，中文能力优秀
- 长文本处理能力强
- 适合文档分析

### OpenRouter 配置

**获取API Key**：
1. 访问 [OpenRouter](https://openrouter.ai/)
2. 注册并获取API Key

**推荐配置**：
```
模型: anthropic/claude-3.5-sonnet
温度: 0.7
最大Token: 4000
```

**优势**：
- 一个API访问多个模型
- 包含Claude、Llama等热门模型
- 灵活的计费方式

## 🎮 使用体验

### 在策略制定中选择模型

1. 进入 **策略制定** 页面
2. 填写营销目标和描述
3. 在 **选择AI模型** 部分选择要使用的配置
4. 点击 **制定传播策略** 生成结果

### 模型选择建议

**快速原型** → DeepSeek Chat (成本低)
**高质量内容** → GPT-4o (质量高)
**长文档处理** → Gemini 1.5 Pro (上下文长)
**中文内容** → Moonshot v1 (中文优化)
**多样化需求** → OpenRouter (模型丰富)

## 📊 使用统计

LLM设置页面提供详细的使用统计：

- **活跃配置数**：当前可用的模型配置
- **总请求数**：累计API调用次数
- **总花费**：累计API使用成本
- **平均响应时间**：模型响应速度

## 🔧 高级功能

### 自定义配置

**自定义名称**：
```
示例：
- "GPT-4 高质量版本"
- "DeepSeek 代码专用"
- "本地部署 Llama"
```

**自定义模型ID**：
```
支持场景：
- 新发布的模型版本
- 微调后的专用模型
- 私有部署的模型
- 第三方代理服务
```

**自定义API URL**：
```
使用场景：
- 代理服务：https://api.proxy.com/v1
- 私有部署：https://your-domain.com/api/v1
- 镜像服务：https://mirror.openai.com/v1
- 本地服务：http://localhost:8000/v1
```

### 批量管理

- **启用/停用**：快速控制配置可用性
- **设为默认**：更改系统默认模型
- **删除配置**：移除不需要的配置
- **编辑配置**：修改现有配置参数

### 安全功能

- **API Key隐藏**：默认隐藏敏感信息
- **连接测试**：验证配置有效性
- **错误提示**：详细的错误信息

### 成本控制

- **实时统计**：监控API使用成本
- **模型对比**：不同模型的价格对比
- **使用建议**：根据需求推荐合适模型

## 💡 最佳实践

### 1. 多模型配置策略

```
日常使用: DeepSeek Chat (成本低)
重要任务: GPT-4o (质量高)  
长文本: Gemini 1.5 Pro (上下文长)
备用方案: OpenRouter (稳定性)
```

### 2. 参数调优建议

**创意内容生成**：
- 温度: 0.8-1.0 (更有创意)
- 最大Token: 2000-4000

**分析报告生成**：
- 温度: 0.3-0.5 (更准确)
- 最大Token: 4000-8000

**代码生成**：
- 温度: 0.1-0.3 (更精确)
- 最大Token: 2000-4000

### 3. 成本优化

1. **开发测试**：使用DeepSeek或GPT-3.5
2. **生产环境**：根据质量要求选择
3. **批量处理**：选择成本效益最高的模型
4. **定期检查**：监控使用统计，优化配置

## 🚨 常见问题

### Q: API Key无效怎么办？
**A**: 检查API Key是否正确复制，是否有足够余额，是否在正确的服务商平台获取。

### Q: 连接测试失败？
**A**: 检查网络连接，确认API Key权限，查看错误提示信息。

### Q: 如何选择合适的模型？
**A**: 根据任务类型、质量要求、成本预算和响应速度需求综合考虑。

### Q: 可以同时使用多个模型吗？
**A**: 可以配置多个模型，在策略制定时选择使用哪个。

### Q: 如何控制成本？
**A**: 选择合适的模型，设置合理的Token限制，定期检查使用统计。

## 🔄 更新计划

即将推出的功能：
- **自动模型选择**：根据任务类型自动推荐模型
- **成本预警**：设置使用限额和提醒
- **性能对比**：不同模型的效果对比
- **批量配置**：快速导入/导出配置

现在就开始配置您的第一个LLM服务，体验多模型的强大能力！🚀
